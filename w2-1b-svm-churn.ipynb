{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines: Churn Analysis\n",
    "\n",
    "Let's look at a classification example in Python.  We are going to look at some telecom data to see whether or not a customer \"churned\" or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "dataset = pd.read_csv(\"https://s3.amazonaws.com/elephantscale-public/data/churn/telco.csv.gz\")\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(len(dataset), (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Basic Data Analytics\n",
    "Let's see how the data is spread along some columns : Churn, Gender, Contract.\n",
    "\n",
    "Do you think the data has skew?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## distribution buy Chrun\n",
    "dataset.groupby('Churn').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Distribution by gender\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : distribution by 'Contract'\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic describe\n",
    "## TODO : Feel free to add more attributes to describe\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define columns\n",
    "prediction = 'Churn'\n",
    "categorical = ['gender',  'InternetService','Contract','PaymentMethod']\n",
    "categorical_index = ['gender_index',  'InternetService_index','Contract_index','PaymentMethod_index']\n",
    "\n",
    "\n",
    "columns = ['SeniorCitizen','PhoneService','Partner','Dependents','tenure','MultipleLines',\n",
    "           'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',\n",
    "           'StreamingTV','StreamingMovies','PaperlessBilling',\n",
    "           'MonthlyCharges','TotalCharges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deal with Categorical Columns\n",
    "\n",
    "Let's deal with the categorical columns, including the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical:\n",
    "    dataset[column + \"_index\"] = pd.factorize(dataset[column])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build the Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(dataset[columns + categorical_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Split into training and test.\n",
    "\n",
    "**=> Split into training/test with an 80/20 split ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## Split into training and test\n",
    "## TODO: create training and test with an 80/20 split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_data, dataset[prediction] == \"Yes\", test_size=0.2)\n",
    "\n",
    "\n",
    "print (\"training set count \", len(X_train))\n",
    "print (\"testing set count \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train  SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC().fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9:  Predict on Test Data\n",
    "\n",
    "**=> TODO: Transform the test dataset to get scaled Vector **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: See the evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> What does AUC mean?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 : confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: What is the meaning of the confusion matrix? **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 11: Try running without scaling features\n",
    "\n",
    "In Step-5  we are adding a scaler at the end to normalize the vector.  \n",
    "Try without scaler.  \n",
    "\n",
    "Uncomment the following line   \n",
    "`#scaled_data = scaler.fit_transform(dataset[columns + categorical_index])`\n",
    "\n",
    "And replace it with something like this\n",
    "\n",
    "```python\n",
    "scaled_data = dataset[columns + categorical_index]\n",
    "```\n",
    "\n",
    "And run the whole notebook (Cell --> Run All)  \n",
    "Do you see any improvement/degradation in accuracy / AUC ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
